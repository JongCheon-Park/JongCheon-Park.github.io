---
layout: page
title: Projects
excerpt: "A List of Projects"
comments: false
---

---

<span style="color:Salmon"> Click the titles to see the details </span>

<br>
<details>
  <summary> <span style="font-size: 20px"> (2021.04~2021.11) 다관절 로봇의 물리적 지능을 위한 교시학습 인공지능기술개발 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2020.09~) 로봇 매니퓰레이터를 위한 모방학습 방법 [Paper] - Under Review </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2020.04~2020.11) 다관절 로봇을 위한 인공지능 모방학습 기술 개발 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2019.04~2019.11) 로봇의 센서리스 기반 외력 추정 및 임피던스 제어 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2019.03~2019.11) 지능형 이동로봇 추적제어 알고리즘 구현 및 실험 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
<br>

---
<br>
<details>
  <summary> <span style="font-size: 20px"> (2018.06~2018.11) 2018 R-BIZ challenge  터틀봇3 오토레이스 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
    <div style="border: 1px solid black; padding: 10px; border-color: LightGray; background-color: rgba(211, 211, 211, 0.2);"> 
      <span style="font-size: 13px">
          2018 R-BIZ Challenge 터틀봇 3 오토레이스는 주어진 6개의 미션 (1. 신호등 미션, 2. 삼거리 미션, 3. 공사구간 미션,<br> 
          4. 주차 미션, 5. 차단 바 미션, 6. 터널 미션)을 수행하며 사전에 설계된 경기장을 1회 주행하는 것을 목표로 한다.
        </span>
    </div>
</p>


<br>
<p style="font-size:1.5rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Turtlebot3 AutoRacing Missions
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/traffic.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/intersection.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/obstacle.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/parking.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/stop.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/tunnel.JPG">
</p>
                                                                      
<p align="center">
    <img height="250" src="/assets/img/Turtlebot3_AutoRacing/racing.gif">
    <p style="text-align:center;">그림 1. 대회주행 영상</p>
</p>
                                 
                                 
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/award.jpg">
<p style="text-align:center;"> 학부생 (<b>박종천</b>, 이응창, 김범주, 대학원생 진용식)으로 이루어진 '응창호'팀이 11월 15~17일 3일간 대구 엑스코 전시장에서 열린 '2018 대구국제로봇산업전 R-BIZ Challenge 터틀봇3 오토레이스'부문에서 16일 준결승전에서 4등을 기록하는 등 <b>'매스웍스 코리아 대표이사상'(상금 100만원 등)</b>을 수상</p>
</p>
</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2018.06~2018.11) 다중주기 센서융합 기반 이동체 실시간 예측 제어 연구 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)<br>
      본 연구사업은 과학기술정보통신부의 출연금 등으로 수행하고 있는 <b>한국전자통신연구원의 대경권 지역산업 기반 ICT 융합기술 고도화 지원사업 위탁연구과제</b>입니다.

  </div>  
    
  <br>
    <span style="font-size: 20px; color:black"> 연구목표 </span><br>
    <div style="border: 1px solid black; padding: 10px; border-color: LightGray; background-color: rgba(211, 211, 211, 0.2);"> 
      <span style="font-size: 13px">
        　■ 　다양한 샘플링 주기를 가지는 시스템의 안정성 및 제어 성능 향상을 위한 기법 연구 <br>
         　■ 　다중 샘플링 주기를 가지는 샘플 데이터 시스템을 위한 상태 추정기 설계 <br>
         　■ 　불확실성 및 왜란에 대응한 실시간 동작을 위한 모델 기반 예측 제어기 설계<br>
         　■ 　<b>[맡은 역할] 모바일 로봇을 위한 라이다와 카메라센서 기반의 Path Planning 알고리즘 구현 및 적용</b><br>
         　■ 　<b>[맡은 역할] ROS 기반의 모바일 로봇 제어 시스템 구축</b><br>
        </span>
    </div>
    <br>
<br>
<p style="font-size:1.5rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　모바일 로봇을 위한 라이다와 카메라센서 기반의 Path Planning 알고리즘 구현 및 적용
</p>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Localization을 위한 Zed 카메라 Visual Odometry 활용
</p>
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/visual_odom.png">
</p>
Visual Odometry 측정 과정<br>
■ 스트레오 카메라를 이용한 영상 입력<br>
■ 왜곡 보정과 수정을 거친 후 거리 측정<br>
■ 특징 찾기와 매칭<br>
■ 병진 그리고 회전 행렬을 추정<br>
Zed Camera는 해당과정을 이용한 Visual Odometry를 제공한다.<br>
    
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/velodyne_lidar.png">
  <img height="250" src="/assets/img/2018_ETRI/lidar_data.png">
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/path_planning.png">
</p>
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/ros_data.png">
</p>
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/autonomous.png">
</p>
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/control_system.png">
</p>
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/husky-experiment.gif">
    <p style="text-align:center;">그림 1. Husky로봇 야외 실험영상</p>
</p>
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/husky-experiment2.gif"> 
    <p style="text-align:center;">그림 2. 라이다 센서 기반의 Path planning </p>
</p>
    
    
</details>
    
<br>

---
