---
layout: page
title: Projects
excerpt: "A List of Projects"
comments: false
---

---

<span style="color:Salmon"> Click the titles to see the details </span>

<br>
<details>
  <summary> <span style="font-size: 20px"> (2021.04~2021.11) 다관절 로봇의 물리적 지능을 위한 교시학습 인공지능기술개발 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2020.09~) 로봇 매니퓰레이터를 위한 모방학습 방법 [Paper] - Under Review </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2020.04~2020.11) 다관절 로봇을 위한 인공지능 모방학습 기술 개발 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2019.04~2019.11) 로봇의 센서리스 기반 외력 추정 및 임피던스 제어 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2019.03~2019.11) 지능형 이동로봇 추적제어 알고리즘 구현 및 실험 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　팔로워 로봇은 카메라를 이용하여 리더 로봇를 인식하고, 인식된 팔로워의 속도 및 각속도를 설계된 알고리즘에 의해 추정한다. 해당 프로젝트는 추정된 속도 및 각속도를 활용하여 리더-팔로워 추적 제어 구현을 목표로 한다.
</p>

<br>
<span style="font-size: 20px; color:black"> Leader-Follower 추적 제어를 위한 기구학적 모델링 </span><br>
    
<p align="center">
  <img height="250" src="/assets/img/Leader_follower/state_equation.png">
    <p style="text-align:center;">Leader-Follwer 추적 제어 시스템을 위한 상태 방정식</p>
</p>
<br>
<span style="font-size: 20px; color:black"> Gazebo 상에서 구현된 가상의 Leader 추적 제어 </span><br>
<p align="center">
  <img height="250" src="/assets/img/Leader_follower/gazebo_simulation.gif">
    <p style="text-align:center;">Leader-Follwer 추적 제어 시스템을 사용한 가상의 Leader 로봇 추적 시뮬레이션<br>빨간선 - 가상의 Leader의 궤적, 파란선 - Follwer의 궤적</p>
</p>
<br>
<span style="font-size: 20px; color:black"> Leader 로봇 인식과 속도 및 각속도 추정 </span><br>
<p align="center">
  <img height="250" src="/assets/img/Leader_follower/yolov2.gif">
    <p style="text-align:center;">YOLOv2를 사용한 Leader 로봇 인식</p>
</p>
<br>
<span style="font-size: 15px; color:black"> 1. Leader 로봇을 인식하고 Depth 카메라를 이용하여 거리값을 측정한다.<br> 2. Localization을 통하여 Global 맵상에 Leader 로봇의 궤적을 기록한다.<br> 3. 궤적중 여러 포인트를 샘플링하고 평균필터를 이용하여 리더 로봇의 각도와 각속도를 추정한다.<br> 4. 실험을 통해 측정한 결과 평균오차 4%~6%를 기록하였다. </span><br>
<br>
<span style="font-size: 20px; color:black"> 실제 실험을 통한 검증 </span><br>
<p align="center">
  <img height="250" src="/assets/img/Leader_follower/experiment.gif">
    <p style="text-align:center;">실제 실험을 통한 각도 및 각속도 추정기반의 추적 제어 알고리즘 성능 검증<br>Leader 로봇은 선속도 0.2m/s, 각속도 0.15rad/s로 움직인다.</p>
</p>
</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2018.06~2018.11) 2018 R-BIZ challenge  터틀봇3 오토레이스 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
    <div style="border: 1px solid black; padding: 10px; border-color: LightGray; background-color: rgba(211, 211, 211, 0.2);"> 
      <span style="font-size: 13px">
          2018 R-BIZ Challenge 터틀봇 3 오토레이스는 주어진 6개의 미션 (1. 신호등 미션, 2. 삼거리 미션, 3. 공사구간 미션,<br> 
          4. 주차 미션, 5. 차단 바 미션, 6. 터널 미션)을 수행하며 사전에 설계된 경기장을 1회 주행하는 것을 목표로 한다.
        </span>
    </div>
</p>


<br>
<p style="font-size:1.5rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Turtlebot3 AutoRacing Missions
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/traffic.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/intersection.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/obstacle.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/parking.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/stop.JPG">
</p>
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/tunnel.JPG">
</p>
                                                                      
<p align="center">
    <img height="250" src="/assets/img/Turtlebot3_AutoRacing/racing.gif">
    <p style="text-align:center;">그림 1. 대회주행 영상</p>
</p>
                                 
                                 
<p align="center">
  <img height="250" src="/assets/img/Turtlebot3_AutoRacing/award.jpg">
<p style="text-align:center;"> 학부생 (<b>박종천</b>, 이응창, 김범주, 대학원생 진용식)으로 이루어진 '응창호'팀이 11월 15~17일 3일간 대구 엑스코 전시장에서 열린 '2018 대구국제로봇산업전 R-BIZ Challenge 터틀봇3 오토레이스'부문에서 16일 준결승전에서 4등을 기록하는 등 <b>'매스웍스 코리아 대표이사상'(상금 100만원 등)</b>을 수상</p>
</p>
</details>
    
<br>

---

<br>
<details>
  <summary> <span style="font-size: 20px"> (2018.06~2018.11) 다중주기 센서융합 기반 이동체 실시간 예측 제어 연구 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)<br>
      본 연구사업은 과학기술정보통신부의 출연금 등으로 수행하고 있는 <b>한국전자통신연구원의 대경권 지역산업 기반 ICT 융합기술 고도화 지원사업 위탁연구과제</b>입니다.

  </div>  
    
  <br>
    <span style="font-size: 20px; color:black"> 연구목표 </span><br>
    <div style="border: 1px solid black; padding: 10px; border-color: LightGray; background-color: rgba(211, 211, 211, 0.2);"> 
      <span style="font-size: 13px">
        　■ 　다양한 샘플링 주기를 가지는 시스템의 안정성 및 제어 성능 향상을 위한 기법 연구 <br>
         　■ 　다중 샘플링 주기를 가지는 샘플 데이터 시스템을 위한 상태 추정기 설계 <br>
         　■ 　불확실성 및 왜란에 대응한 실시간 동작을 위한 모델 기반 예측 제어기 설계<br>
         　■ 　<b>[맡은 역할] 모바일 로봇을 위한 라이다센서 기반의 Path Planning 알고리즘 구현 및 적용</b><br>
         　■ 　<b>[맡은 역할] ROS 기반의 모바일 로봇 제어 시스템 구축</b><br>
        </span>
    </div>
    <br>
<br>
<p style="font-size:1.5rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
라이다센서 기반의 Path Planning 알고리즘 구현 및 적용
</p>

<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/vlp16.png">
    <p style="text-align:center;">라이다는 고출력의 펄스레이저를 이용하여 물체에 반사되어 돌아오는 레이저 빔의 시간을 측정하여 거리정보를 획득
카메라센서만으로 측정이 어려운 거리정보나, 비가 오거나 어두운 환경에서도 주변의 물체를 인식할 수 있다.</p>
</p>

<p style="font-size:1.5rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
    <b>LiDAR를 이용한 Obstacle detection, Mapping과 A*기반의 Path Planning</b>
</p>

<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/path_planning.png">
</p>
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/path_planning2.png">
</p>
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/path_planning3.png">
</p>

<p style="font-size:1.5rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
    <b>LiDAR를 이용한 Obstacle detection, Mapping과 A*기반의 Path Planning</b>
</p>

<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/autonomous.png">
  <p style="text-align:center;">ROS 기반의 autonomous vehicle control system</p>
</p>

<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/husky-experiment.gif">
    <p style="text-align:center;">Husky로봇 야외 실험영상</p>
</p>
<p align="center">
  <img height="250" src="/assets/img/2018_ETRI/husky-experiment2.gif"> 
    <p style="text-align:center;">라이다 센서 기반의 Path planning </p>
</p>
    
    
</details>
    
<br>

---
